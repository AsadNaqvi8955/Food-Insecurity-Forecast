{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3941698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "5a222474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FA_09 = pd.read_excel('FI_2009.xlsx')\n",
    "df_FA_10 = pd.read_excel('FI_2010.xlsx')\n",
    "df_FA_11 = pd.read_excel('FI_2011.xlsx') # data only provided at the state level\n",
    "df_FA_12 = pd.read_excel('FI_2012.xlsx') # data only provided at the state level\n",
    "df_FA_13 = pd.read_excel('FI_2013.xlsx') ## data only provided at the state level\n",
    "df_FA_14 = pd.read_excel('FI_2014.xlsx')\n",
    "df_FA_15 = pd.read_excel('FI_2015.xlsx') \n",
    "df_FA_16 = pd.read_excel('FI_2016.xlsx')\n",
    "df_FA_17 = pd.read_excel('FI_2017.xlsx')\n",
    "df_FA_18 = pd.read_excel('FI_2018.xlsx')\n",
    "df_FA_19 = pd.read_excel('FI_2019.xlsx') # contains data 2019,2020,2021,2022,2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "3e0d40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FA_09 = df_FA_09.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_10 = df_FA_10.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_11 = df_FA_11.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_12 = df_FA_12.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_13 = df_FA_13.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_14 = df_FA_14.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_15 = df_FA_15.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_16 = df_FA_16.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_17 = df_FA_17.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_18 = df_FA_18.drop(['State Name', 'County Code','Low Threshold in state', 'High Threshold in state', '% FI ≤ Low Threshold',\n",
    "            '% FI Btwn Thresholds','% FI > High Threshold', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Number Food Insecure Children','% of children in FI HH with HH incomes above 185% FPL'], axis=1)\n",
    "\n",
    "df_FA_19 = df_FA_19.drop(['State Name', 'County Code','Food Insecurity Rate among Black Persons (all ethnicities)',\n",
    "       'Food Insecurity Rate among Hispanic Persons (any race)',\n",
    "       'Food Insecurity Rate among White, non-Hispanic Persons', '% FI ≤ SNAP Threshold',\n",
    "            '% FI > SNAP Threshold','Number Food Insecure Children', '% of children in FI HH with HH incomes at or below 185% FPL',\n",
    "              'Weighted weekly $ needed by FI','% of children in FI HH with HH incomes above 185% FPL','Rural-Urban Continuum Code (2013)','Rural-Urban Continuum Code (2023)', 'Census Region', 'Census Division',\n",
    "       'FNS Region','Low Threshold in state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "139fef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FA_09['Year'] = '2009'\n",
    "df_FA_10['Year'] = '2010'\n",
    "df_FA_11['Year'] = '2011'\n",
    "df_FA_12['Year'] = '2012'\n",
    "df_FA_13['Year'] = '2013'\n",
    "df_FA_14['Year'] = '2014'\n",
    "df_FA_15['Year'] = '2015'\n",
    "df_FA_16['Year'] = '2016'\n",
    "df_FA_17['Year'] = '2017'\n",
    "df_FA_18['Year'] = '2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0ae6f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FIPS  FI Rate Low Threshold Type  High Threshold Type  Cost Per Meal  Year\n",
      "0  02013    0.151               SNAP  other nutrition pgm            NaN  2009\n",
      "1  02016    0.136               SNAP  other nutrition pgm            NaN  2009\n",
      "2  02020    0.119               SNAP  other nutrition pgm            NaN  2009\n",
      "3  02050    0.211               SNAP  other nutrition pgm            NaN  2009\n",
      "4  02060    0.095               SNAP  other nutrition pgm            NaN  2009\n"
     ]
    }
   ],
   "source": [
    "df_FA = pd.concat([df_FA_09, df_FA_10,df_FA_11,df_FA_12,df_FA_13,df_FA_14,df_FA_15,df_FA_16,df_FA_17,\n",
    "    df_FA_18,df_FA_19 ]).drop(['Weighted Annual Dollars', 'Number Food Insecure Individuals','Child FI Rate'],axis=1)\n",
    "\n",
    "df_FA.drop(df_FA[df_FA['FIPS'].isnull()].index, axis=0, inplace=True)\n",
    "df_FA['FIPS'] = np.where(df_FA['FIPS']<10000, \n",
    "                        '0'+df_FA['FIPS'].astype(int).astype(str), df_FA['FIPS'].astype(int).astype(str))\n",
    "df_FA.to_csv('output.csv', index=False)\n",
    "print(df_FA.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "eb3a0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Zillow dataset\n",
    "df_rent = pd.read_csv('Rent Prices/rent_prices.csv')\n",
    "\n",
    "# isolate columns corresponding to each year, and make new column\n",
    "drop_00 = df_rent.columns[df_rent.columns.str.contains('2000')]\n",
    "drop_01 = df_rent.columns[df_rent.columns.str.contains('2001')]\n",
    "drop_02 = df_rent.columns[df_rent.columns.str.contains('2002')]\n",
    "drop_03 = df_rent.columns[df_rent.columns.str.contains('2003')]\n",
    "drop_04 = df_rent.columns[df_rent.columns.str.contains('2004')]\n",
    "drop_05 = df_rent.columns[df_rent.columns.str.contains('2005')]\n",
    "drop_06 = df_rent.columns[df_rent.columns.str.contains('2006')]\n",
    "drop_07 = df_rent.columns[df_rent.columns.str.contains('2007')]\n",
    "drop_08 = df_rent.columns[df_rent.columns.str.contains('2008')]\n",
    "drop_25 = df_rent.columns[df_rent.columns.str.contains('2025')]\n",
    "#drop unnecessary rent data (Years -2000,2001,2002,2003,2004,2005,2006,2007,2008,2025)\n",
    "drop_unnecessary_years = drop_00.append(drop_01).append(drop_02).append(drop_03).append(drop_04).append(drop_05).append(drop_06).append(drop_07).append(drop_08).append(drop_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f6e15319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RegionID  SizeRank       RegionName RegionType StateName           2009  \\\n",
      "0    102001         0    United States    country       NaN  178469.149291   \n",
      "1    394913         1     New York, NY        msa        NY  412952.308785   \n",
      "2    753899         2  Los Angeles, CA        msa        CA  405039.690092   \n",
      "3    394463         3      Chicago, IL        msa        IL  215000.507259   \n",
      "4    394514         4       Dallas, TX        msa        TX  150500.080957   \n",
      "\n",
      "            2010           2011           2012           2013           2014  \\\n",
      "0  172357.710300  163357.042347  161978.036844  172315.087885  183951.481975   \n",
      "1  401317.370400  389149.285754  379994.823081  389054.232434  409133.454829   \n",
      "2  403957.248479  381474.610494  372240.603089  429955.987603  482992.579917   \n",
      "3  200001.943217  182099.815957  170286.516361  177744.636048  194652.143347   \n",
      "4  149918.840404  144138.705114  145773.446983  156254.677501  169269.453211   \n",
      "\n",
      "            2015           2016           2017           2018           2019  \\\n",
      "0  193744.356840  205577.099688  217898.575402  231744.531403  243352.079716   \n",
      "1  422747.458155  441893.117852  464036.997559  486740.551409  503603.784239   \n",
      "2  507500.823312  526872.733944  573679.046262  633652.007859  649412.485284   \n",
      "3  203108.251614  213734.429537  224854.022049  235544.995232  241389.346642   \n",
      "4  185696.680140  207738.105553  229310.141546  247989.117645  256578.530251   \n",
      "\n",
      "            2020           2021           2022           2023           2024  \n",
      "0  258493.860201  297504.753789  341790.049438  349978.680686  361630.250314  \n",
      "1  520740.628677  569693.613740  617241.770614  631268.022953  673771.502066  \n",
      "2  682947.724685  777965.765704  882556.170350  881352.039929  940500.529471  \n",
      "3  248189.573546  274304.031786  298751.058707  307379.568650  326183.864738  \n",
      "4  267911.418555  310388.593547  376490.967722  378577.915256  380382.041476  \n"
     ]
    }
   ],
   "source": [
    "#take mean rent value of 2009 from monthly rent data \n",
    "drop_09 = df_rent.columns[df_rent.columns.str.contains('2009')]\n",
    "df_rent['2009'] = df_rent.loc[:,drop_09].mean(axis=1)\n",
    "\n",
    "drop_10 = df_rent.columns[df_rent.columns.str.contains('2010')]\n",
    "df_rent['2010'] = df_rent.loc[:,drop_10].mean(axis=1)\n",
    "\n",
    "drop_11 = df_rent.columns[df_rent.columns.str.contains('2011')]\n",
    "df_rent['2011'] = df_rent.loc[:,drop_11].mean(axis=1)\n",
    "\n",
    "drop_12 = df_rent.columns[df_rent.columns.str.contains('2012')]\n",
    "df_rent['2012'] = df_rent.loc[:,drop_12].mean(axis=1)\n",
    "\n",
    "drop_13 = df_rent.columns[df_rent.columns.str.contains('2013')]\n",
    "df_rent['2013'] = df_rent.loc[:,drop_13].mean(axis=1)\n",
    "\n",
    "drop_14 = df_rent.columns[df_rent.columns.str.contains('2014')]\n",
    "df_rent['2014'] = df_rent.loc[:,drop_14].mean(axis=1)\n",
    "\n",
    "drop_15 = df_rent.columns[df_rent.columns.str.contains('2015')]\n",
    "df_rent['2015'] = df_rent.loc[:,drop_15].mean(axis=1)\n",
    "\n",
    "drop_16 = df_rent.columns[df_rent.columns.str.contains('2016')]\n",
    "df_rent['2016'] = df_rent.loc[:,drop_16].mean(axis=1)\n",
    "\n",
    "drop_17 = df_rent.columns[df_rent.columns.str.contains('2017')]\n",
    "df_rent['2017'] = df_rent.loc[:,drop_17].mean(axis=1)\n",
    "\n",
    "drop_18 = df_rent.columns[df_rent.columns.str.contains('2018')]\n",
    "df_rent['2018'] = df_rent.loc[:,drop_18].mean(axis=1)\n",
    "\n",
    "drop_19 = df_rent.columns[df_rent.columns.str.contains('2019')]\n",
    "df_rent['2019'] = df_rent.loc[:,drop_19].mean(axis=1)\n",
    "\n",
    "drop_20 = df_rent.columns[df_rent.columns.str.contains('2020')]\n",
    "df_rent['2020'] = df_rent.loc[:,drop_20].mean(axis=1)\n",
    "\n",
    "drop_21 = df_rent.columns[df_rent.columns.str.contains('2021')]\n",
    "df_rent['2021'] = df_rent.loc[:,drop_21].mean(axis=1)\n",
    "\n",
    "drop_22 = df_rent.columns[df_rent.columns.str.contains('2022')]\n",
    "df_rent['2022'] = df_rent.loc[:,drop_22].mean(axis=1)\n",
    "\n",
    "drop_23 = df_rent.columns[df_rent.columns.str.contains('2023')]\n",
    "df_rent['2023'] = df_rent.loc[:,drop_23].mean(axis=1)\n",
    "\n",
    "drop_24 = df_rent.columns[df_rent.columns.str.contains('2024')]\n",
    "df_rent['2024'] = df_rent.loc[:,drop_24].mean(axis=1)\n",
    "\n",
    "# drop all monthly data \n",
    "to_drop = drop_09.append(drop_10).append(drop_11).append(drop_12).append(drop_13).append(drop_14).append(drop_15).append(drop_16).append(drop_17).append(drop_18).append(drop_19).append(drop_20).append(drop_21).append(drop_22).append(drop_23).append(drop_24)\n",
    "df_rent.drop(to_drop, axis=1, inplace=True)\n",
    "df_rent.drop(drop_unnecessary_years, axis=1, inplace=True)\n",
    "print(df_rent.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "327b7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FIPS  RegionID  SizeRank           2009           2010           2011  \\\n",
      "0  01003  394519.0     203.0  207376.937658  186818.104910  170807.887949   \n",
      "1  01015  394333.0     357.0            NaN  109196.981556  105914.893866   \n",
      "2  01031  394571.0     585.0            NaN            NaN            NaN   \n",
      "3  01043  394511.0     424.0  136801.733401  131884.505736  128362.809448   \n",
      "4  01045  753911.0     608.0  101221.721813   96968.110637   94958.564644   \n",
      "\n",
      "            2012           2013           2014           2015           2016  \\\n",
      "0  170449.578811  181434.478392  190518.914861  201298.759221  212478.487376   \n",
      "1  108283.438311  114486.363002  145233.124551  115019.783267  111659.561606   \n",
      "2  110019.581474  116472.119089  114387.433066  112939.114329  114776.347203   \n",
      "3  128964.899310  129896.808937  136357.607109  143423.412074  152323.947105   \n",
      "4   96524.094802  100902.225769  101416.592325  100517.020426  102021.776179   \n",
      "\n",
      "            2017           2018           2019           2020           2021  \\\n",
      "0  221303.235702  233491.559176  246578.059684  266221.308901  312712.325934   \n",
      "1  111799.980137  116835.609958  125092.829194  131744.160356  147379.971818   \n",
      "2  115246.333452  118511.730613  121178.156493  128678.745380  141244.518217   \n",
      "3  157062.461112  162381.665015  169289.144151  182560.695468  208304.905827   \n",
      "4  101228.864462  106059.127579  110861.715509  119070.440978  129460.101421   \n",
      "\n",
      "            2022           2023           2024  \n",
      "0  362677.643384  377956.913865  382648.930778  \n",
      "1  157447.024789  158855.587965  160881.853718  \n",
      "2  155069.951256  163074.091282  169769.730083  \n",
      "3  238650.717028  240442.349739  240630.999117  \n",
      "4  141783.545238  151165.075739  157567.775715  \n"
     ]
    }
   ],
   "source": [
    "#dataset with zipcode-to-county mapping\n",
    "zips = pd.read_csv('uszips.csv')\n",
    "zips['RegionName'] = zips['city'] + ', ' + zips['state_id']\n",
    "# drop some columns and rename some\n",
    "zips =zips.loc[:,['RegionName', 'county_fips']]\n",
    "zips.rename(columns={'county_fips': 'FIPS'},inplace=True)\n",
    "# add leading zero to FIPS values where needed\n",
    "zips['FIPS'] = np.where(zips['FIPS']<10000, \n",
    "                        '0'+zips['FIPS'].astype(str), zips['FIPS'].astype(str))\n",
    "new_df = pd.DataFrame({\n",
    "    'colA': df_rent['RegionName'],\n",
    "    'colB': zips['RegionName']\n",
    "})\n",
    "\n",
    "# merge Zillow dataset with zipcodes dataset\n",
    "merged = df_rent.merge(zips, on='RegionName', how='left')\n",
    "merged=merged.drop_duplicates()\n",
    "merged.to_csv('outputdata/data_with_rent.csv', index=False)\n",
    "merged = merged.groupby('FIPS').mean(numeric_only=True).reset_index()\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "b02f1c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FIPS  FI Rate Low Threshold Type  High Threshold Type  Cost Per Meal  \\\n",
      "0  02013    0.151               SNAP  other nutrition pgm            NaN   \n",
      "1  02016    0.136               SNAP  other nutrition pgm            NaN   \n",
      "2  02020    0.119               SNAP  other nutrition pgm            NaN   \n",
      "3  02050    0.211               SNAP  other nutrition pgm            NaN   \n",
      "4  02060    0.095               SNAP  other nutrition pgm            NaN   \n",
      "\n",
      "   Year        Rent  \n",
      "0  2009         NaN  \n",
      "1  2009         NaN  \n",
      "2  2009  2615.96195  \n",
      "3  2009         NaN  \n",
      "4  2009         NaN  \n"
     ]
    }
   ],
   "source": [
    "# 2009\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_09 = merged[['FIPS', '2009']]\n",
    "df_rent_09.rename(columns={'2009':'Rent'}, inplace=True)\n",
    "df_rent_09['Year'] = '2009'\n",
    "\n",
    "# 2010\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_10 = merged[['FIPS', '2010']]\n",
    "df_rent_10.rename(columns={'2010':'Rent'}, inplace=True)\n",
    "df_rent_10['Year'] = '2010'\n",
    "\n",
    "# 2011\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_11 = merged[['FIPS', '2011']]\n",
    "df_rent_11.rename(columns={'2011':'Rent'}, inplace=True)\n",
    "df_rent_11['Year'] = '2011'\n",
    "\n",
    "# 2012\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_12 = merged[['FIPS', '2012']]\n",
    "df_rent_12.rename(columns={'2012':'Rent'}, inplace=True)\n",
    "df_rent_12['Year'] = '2012'\n",
    "\n",
    "# 2013\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_13 = merged[['FIPS', '2013']]\n",
    "df_rent_13.rename(columns={'2013':'Rent'}, inplace=True)\n",
    "df_rent_13['Year'] = '2013'\n",
    "\n",
    "# 2014\n",
    "# isolate relevant columns, rename, and creat year column\n",
    "df_rent_14 = merged[['FIPS', '2014']]\n",
    "df_rent_14.rename(columns={'2014':'Rent'}, inplace=True)\n",
    "df_rent_14['Year'] = '2014'\n",
    "\n",
    "# 2015\n",
    "df_rent_15 = merged[['FIPS', '2015']]\n",
    "df_rent_15.rename(columns={'2015':'Rent'}, inplace=True)\n",
    "df_rent_15['Year'] = '2015'\n",
    "\n",
    "# 2016\n",
    "df_rent_16 = merged[['FIPS', '2016']]\n",
    "df_rent_16.rename(columns={'2016':'Rent'}, inplace=True)\n",
    "df_rent_16['Year'] = '2016'\n",
    "\n",
    "\n",
    "# 2017\n",
    "df_rent_17 = merged[['FIPS', '2017']]\n",
    "df_rent_17.rename(columns={'2017':'Rent'}, inplace=True)\n",
    "df_rent_17['Year'] = '2017'\n",
    "\n",
    "# 2018\n",
    "df_rent_18 = merged[['FIPS', '2018']]\n",
    "df_rent_18.rename(columns={'2018':'Rent'}, inplace=True)\n",
    "df_rent_18['Year'] = '2018'\n",
    "\n",
    "# 2019\n",
    "df_rent_19 = merged[['FIPS', '2019']]\n",
    "df_rent_19.rename(columns={'2019':'Rent'}, inplace=True)\n",
    "df_rent_19['Year'] = '2019'\n",
    "\n",
    "# 2020\n",
    "df_rent_20 = merged[['FIPS', '2020']]\n",
    "df_rent_20.rename(columns={'2020':'Rent'}, inplace=True)\n",
    "df_rent_20['Year'] = '2020'\n",
    "\n",
    "# 2021\n",
    "df_rent_21 = merged[['FIPS', '2021']]\n",
    "df_rent_21.rename(columns={'2021':'Rent'}, inplace=True)\n",
    "df_rent_21['Year'] = '2021'\n",
    "\n",
    "# 2022\n",
    "df_rent_22 = merged[['FIPS', '2022']]\n",
    "df_rent_22.rename(columns={'2022':'Rent'}, inplace=True)\n",
    "df_rent_22['Year'] = '2022'\n",
    "\n",
    "# 2023\n",
    "df_rent_23 = merged[['FIPS', '2023']]\n",
    "df_rent_23.rename(columns={'2023':'Rent'}, inplace=True)\n",
    "df_rent_23['Year'] = '2023'\n",
    "\n",
    "# 2024\n",
    "df_rent_24 = merged[['FIPS', '2024']]\n",
    "df_rent_24.rename(columns={'2024':'Rent'}, inplace=True)\n",
    "df_rent_24['Year'] = '2024'\n",
    "\n",
    "merged = pd.concat([df_rent_09,df_rent_10,df_rent_11,df_rent_12,df_rent_13,df_rent_14,df_rent_15,df_rent_16,df_rent_17,df_rent_18,df_rent_19,df_rent_20,df_rent_21,df_rent_22,df_rent_23,df_rent_24])\n",
    "merged.drop(merged[merged['FIPS'].isnull()].index, axis=0, inplace=True)\n",
    "\n",
    "merged['FIPS'] = merged['FIPS'].astype(str).str.extract('(\\d+)')[0]\n",
    "merged['FIPS'] = merged['FIPS'].str.zfill(5)\n",
    "df_FIRate_Rent_Added = df_FA.merge(merged, on=['FIPS','Year'],how='outer')\n",
    "df_FIRate_Rent_Added['Rent'] = pd.to_numeric(df_FIRate_Rent_Added['Rent'], errors='coerce')  # convert non-numeric to NaN\n",
    "df_FIRate_Rent_Added['Rent'] = df_FIRate_Rent_Added['Rent'] / 100\n",
    "print(df_FIRate_Rent_Added.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414ee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 45221\n",
      "31823\n"
     ]
    }
   ],
   "source": [
    "#load data (2009 to 2020)\n",
    "file_path = 'fully_cleaned_data .pickle'\n",
    "df = pd.read_pickle(file_path)\n",
    "print(\"Total duplicate rows:\", df.duplicated().sum())\n",
    "df=df.drop_duplicates()\n",
    "\n",
    "#filter data (2021 to 2024)\n",
    "df_2021_to_2024 = df_FIRate_Rent_Added[df_FIRate_Rent_Added['Year'].isin([2021, 2022, 2023, 2024])]\n",
    "df_2014_to_2020 = df_FIRate_Rent_Added[df_FIRate_Rent_Added['Year'].isin([2014, 2015, 2016, 2017, 2018, 2019, 2020])]\n",
    "\n",
    "\n",
    "\n",
    "#print(df_2021_to_2024.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "0940e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 0\n",
      "               Rent  Houseless_rate  Sheltered_rate  Unsheltered_rate  \\\n",
      "count   2766.000000    31698.000000    31698.000000      31698.000000   \n",
      "mean    1549.604889        0.001079        0.000675          0.000403   \n",
      "std     1090.542237        0.001075        0.000571          0.000824   \n",
      "min      619.333333        0.000017        0.000009          0.000000   \n",
      "25%     1206.234375        0.000574        0.000365          0.000078   \n",
      "50%     1416.550000        0.000770        0.000546          0.000169   \n",
      "75%     1667.530714        0.001175        0.000795          0.000393   \n",
      "max    22528.834158        0.016672        0.011712          0.016259   \n",
      "\n",
      "            TOT_POP      TOT_MALE    TOT_FEMALE     TOT_WHITE     TOT_BLACK  \\\n",
      "count  3.204100e+04  3.204100e+04  3.204100e+04  3.204100e+04  3.204100e+04   \n",
      "mean   1.242006e+05  6.108598e+04  6.311465e+04  9.481159e+04  1.659914e+04   \n",
      "std    4.820872e+05  2.367491e+05  2.453715e+05  3.422393e+05  7.792877e+04   \n",
      "min    8.266667e+01  4.100000e+01  3.500000e+01  2.400000e+01  0.000000e+00   \n",
      "25%    1.120600e+04  5.643667e+03  5.548000e+03  9.314000e+03  1.210000e+02   \n",
      "50%    2.665800e+04  1.329400e+04  1.326933e+04  2.287900e+04  9.050000e+02   \n",
      "75%    7.299900e+04  3.623500e+04  3.661000e+04  6.182100e+04  6.041000e+03   \n",
      "max    1.010571e+07  4.980981e+06  5.125335e+06  7.181207e+06  1.311396e+06   \n",
      "\n",
      "          TOT_NATIVE     TOT_ASIAN   TOT_PACIFIC    TOT_LATINX       FI Rate  \\\n",
      "count   32041.000000  3.204100e+04  32041.000000  3.204100e+04  41359.000000   \n",
      "mean     1523.763418  7.842249e+03    286.003319  2.500515e+04      0.143822   \n",
      "std      7015.576995  6.235770e+04   2427.665751  1.981518e+05      0.041458   \n",
      "min         0.000000  0.000000e+00      0.000000  0.000000e+00      0.022000   \n",
      "25%        65.000000  4.800000e+01      4.000000  3.320000e+02      0.116000   \n",
      "50%       187.000000  1.630000e+02     15.000000  1.063000e+03      0.140000   \n",
      "75%       683.000000  8.610000e+02     64.000000  5.128000e+03      0.167250   \n",
      "max    145996.000000  1.545445e+06  95285.000000  4.899383e+06      0.379000   \n",
      "\n",
      "       Cost Per Meal  Num_wholesale  Num_restaraunts    Num_grocery  \\\n",
      "count   41332.000000   18305.000000     18305.000000   18305.000000   \n",
      "mean        3.020481     136.842775       546.057034     538.856651   \n",
      "std         0.525715    1160.949162      3577.240887    4583.978287   \n",
      "min         0.000000       0.000000         0.000000       0.000000   \n",
      "25%         2.647500       0.000000        20.000000      23.000000   \n",
      "50%         2.905000       9.000000        63.000000      69.000000   \n",
      "75%         3.310000      40.000000       218.000000     214.000000   \n",
      "max         7.890000   59576.000000    133486.000000  255585.000000   \n",
      "\n",
      "       Total_workforce      Employed     Unemployed  Unemployment_rate  \n",
      "count     3.955900e+04  3.955900e+04   39559.000000       39559.000000  \n",
      "mean      6.081516e+04  5.671121e+04    4103.948420           6.764985  \n",
      "std       2.318223e+05  2.146621e+05   18788.498043           3.357674  \n",
      "min       4.300000e+01  3.900000e+01       4.000000           0.625000  \n",
      "25%       5.135500e+03  4.760000e+03     310.000000           4.272500  \n",
      "50%       1.203100e+04  1.114600e+04     815.000000           6.100000  \n",
      "75%       3.429250e+04  3.198920e+04    2270.000000           8.580000  \n",
      "max       5.161400e+06  4.945111e+06  683171.400000          28.900000  \n"
     ]
    }
   ],
   "source": [
    "#merge two dataframes\n",
    "common_cols = [col for col in df_2021_to_2024.columns if col in df.columns]\n",
    "df_2021_to_2024 = df_2021_to_2024[common_cols]\n",
    "Fulldata_updated = pd.concat([df, df_2021_to_2024], ignore_index=True)\n",
    "print(\"Total duplicate rows:\", Fulldata_updated.duplicated().sum())\n",
    "print(Fulldata_updated.describe())\n",
    "Fulldata_updated=Fulldata_updated.drop_duplicates()\n",
    "Fulldata_updated.to_csv('outputdata/FullData.csv', index=False)\n",
    "Fulldata_updated.to_pickle('pickled/fully_cleaned_data_final.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "155fe156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to Google Drive. File ID: 1QunQN72d4boWIf-CCTlPbJX2HNiFfr5X\n"
     ]
    }
   ],
   "source": [
    "# UPLOAD TO GOOGLE DRIVE\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "FOLDER_ID = '1IDTPcY2DffHsGtFFkdoq_FiBakYVc49D'  # paste from Drive URL\n",
    "FILE_PATH = 'pickled/fully_cleaned_data_final.pickle'\n",
    "\n",
    "def upload_to_drive(file_path, folder_id):\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    file_metadata = {\n",
    "        'name': os.path.basename(file_path),\n",
    "        'parents': [folder_id]\n",
    "    }\n",
    "    media = MediaFileUpload(file_path, mimetype='application/octet-stream', resumable=True)\n",
    "\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id',\n",
    "        supportsAllDrives=True\n",
    "    ).execute()\n",
    "\n",
    "    print(f\"File uploaded to Google Drive. File ID: {file.get('id')}\")\n",
    "\n",
    "# call upload\n",
    "upload_to_drive(FILE_PATH, FOLDER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
