{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81da2c7e-4ff0-45bf-9426-c9873c74dbab",
   "metadata": {},
   "source": [
    "# Projecting Food Insecurity Rates in the US by County — Modeling Notebook (Test Year: 2023)\n",
    "\n",
    "Objective:- Predict county-level Food Insecurity (FI) rates using a time-aware split (train: 2010–2022; test: 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5049cac-5a03-4ef4-aca8-81c01166cb4e",
   "metadata": {},
   "source": [
    "#### Modeling approach.\n",
    "We kept a good project’s flow while enforcing time awareness:\n",
    "*  Baselines: State mean (2010–2022) and carry-forward (FI_prev) for 2023 as realistic yardsticks.\n",
    "\n",
    "Linear models:\n",
    "* M1: OLS on all kept features (including FI_prev).\n",
    "* M2: OLS with VIF≤10 on numerics to reduce multicollinearity, plus categoricals.\n",
    "\n",
    "Nonlinear models:\n",
    "\n",
    "* Random Forest, XGBoost, CatBoost using sparse (impute+OHE) preprocessing.\n",
    "\n",
    "* Hybrid stack (optional): Ridge meta-model on out-of-fold predictions + a simple state residual correction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f46be05a-a010-439c-bd72-08292b84a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocess & pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model selection / CV\n",
    "from sklearn.model_selection import GroupKFold, KFold, cross_val_score, train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Stats / VIF\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Boosters (guarded by try/except where used)\n",
    "import xgboost as xgb      \n",
    "from catboost import CatBoostRegressor  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a51ca0-7ba9-43f4-9b34-89e267a3ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths & split config\n",
    "CSV_PATH    = \"engineered_data.csv\"   # <-- adjust if needed\n",
    "TRAIN_START = 2010\n",
    "TEST_YEAR   = 2023\n",
    "TRAIN_END   = TEST_YEAR - 1   # 2022\n",
    "\n",
    "# Feature hygiene\n",
    "DROP_SPARSE = 0.60   # drop features >60% missing in train window\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Small helpers\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(((y_true - y_pred) ** 2).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df10e96-5d16-4c7b-b50f-eae0d39862c7",
   "metadata": {},
   "source": [
    "## Load data & build FI_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cdace-f9a3-482b-800c-0d9cd8537ab0",
   "metadata": {},
   "source": [
    "### Data & Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256df4e9-fb0a-4d36-ba70-d0a036274e0b",
   "metadata": {},
   "source": [
    "We read a single engineered table containing FI Rate (target) and covariates (demographics, labor, cost/access proxies, engineered logs/deltas).\n",
    "To reflect real-world forecasting, we do not shuffle; we split by year:\n",
    "\n",
    "* Train: 2010–2022\n",
    "\n",
    "* Test: 2023\n",
    "\n",
    "We also create FI_prev: the prior-year FI for the same county (FIPS). This captures FI persistence without leaking future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78ed553f-ca14-4f38-9ae5-966b349febb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years present in file: 2009–2023\n",
      "Train: 2010–2022 rows=34534\n",
      "Test : 2023 rows=3130\n",
      "{'train_missing_FI_prev': 0.0005501824289106387, 'test_missing_FI_prev': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Load engineered table\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# IDs\n",
    "df[\"FIPS\"]  = df[\"FIPS\"].astype(str).str.zfill(5)\n",
    "df[\"STATE\"] = df[\"FIPS\"].str[:2]\n",
    "df[\"Year\"]  = df[\"Year\"].astype(int)\n",
    "\n",
    "# Only rows with a labeled target\n",
    "dfm = df[~df[\"FI Rate\"].isna()].copy()\n",
    "\n",
    "# Build lag: previous year's FI per FIPS\n",
    "dfm = dfm.sort_values([\"FIPS\", \"Year\"])\n",
    "dfm[\"FI_prev\"] = dfm.groupby(\"FIPS\")[\"FI Rate\"].shift(1)\n",
    "\n",
    "# Time-aware split\n",
    "train = dfm[(dfm[\"Year\"] >= TRAIN_START) & (dfm[\"Year\"] < TEST_YEAR)].copy()\n",
    "test  = dfm[dfm[\"Year\"] == TEST_YEAR].copy()\n",
    "\n",
    "print(f\"Years present in file: {dfm['Year'].min()}–{dfm['Year'].max()}\")\n",
    "print(f\"Train: {train['Year'].min()}–{train['Year'].max()} rows={len(train)}\")\n",
    "print(f\"Test : {TEST_YEAR} rows={len(test)}\")\n",
    "\n",
    "# Sanity: FI_prev should exist for almost all train rows except the first year per FIPS\n",
    "print({\"train_missing_FI_prev\": float(train[\"FI_prev\"].isna().mean()),\n",
    "       \"test_missing_FI_prev\":  float(test[\"FI_prev\"].isna().mean())})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6123e-1387-4409-b334-b3abff5b0fd9",
   "metadata": {},
   "source": [
    "### Feature Hygiene & Design Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c2d83-3fa3-40c7-84ad-38ccc68a95d7",
   "metadata": {},
   "source": [
    "We exclude identifiers and target: {\"FIPS\",\"STATE\",\"Year\",\"FI Rate\"}. \n",
    "We drop features that are >60% missing in the TRAIN window (to avoid leakage from test’s missingness pattern).\n",
    "We force-keep FI_prev even if sparsity would drop it (it’s critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a38337f-730f-44eb-952c-7bcfffe19fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped sparse (> 60% missing in TRAIN): ['delta_rent', 'food_to_rent_ratio', 'Rent', 'log_rent']\n",
      "Kept: 48 numeric (+FI_prev), 7 categorical = 55 total\n"
     ]
    }
   ],
   "source": [
    "# Separate candidate features by dtype\n",
    "IGNORE = {\"FIPS\",\"STATE\",\"Year\",\"FI Rate\"}\n",
    "num_all = [c for c in dfm.columns if c not in IGNORE and dfm[c].dtype != \"O\"]\n",
    "cat_all = [c for c in dfm.columns if c not in IGNORE and dfm[c].dtype == \"O\"]\n",
    "\n",
    "# compute missingness on TRAIN window\n",
    "miss = train[num_all + cat_all].isna().mean().sort_values(ascending=False)\n",
    "to_drop_sparse = miss[miss > DROP_SPARSE].index.tolist()\n",
    "\n",
    "# base lists\n",
    "numeric_base     = [c for c in num_all if c not in to_drop_sparse]\n",
    "categorical_base = [c for c in cat_all if c not in to_drop_sparse]\n",
    "\n",
    "# --- make sure FI_prev is present even if sparse rule would drop it ---\n",
    "if \"FI_prev\" not in numeric_base and \"FI_prev\" in num_all:\n",
    "    numeric_base = [\"FI_prev\"] + numeric_base  # put it first for clarity\n",
    "\n",
    "X_cols_base = numeric_base + categorical_base\n",
    "\n",
    "print(f\"Dropped sparse (> {int(DROP_SPARSE*100)}% missing in TRAIN): {to_drop_sparse[:12]}{' ...' if len(to_drop_sparse)>12 else ''}\")\n",
    "print(f\"Kept: {len(numeric_base)} numeric (+FI_prev), {len(categorical_base)} categorical = {len(X_cols_base)} total\")\n",
    "\n",
    "# Split matrices\n",
    "X_train_full = train[X_cols_base].copy()\n",
    "y_train      = train[\"FI Rate\"].values\n",
    "X_test_full  = test[X_cols_base].copy()\n",
    "y_test       = test[\"FI Rate\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bd045-63cb-460a-b3ec-0420779e8842",
   "metadata": {},
   "source": [
    "### Preprocessing & Grouped Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7527a3-3718-40de-a7b9-2b3c66e4da09",
   "metadata": {},
   "source": [
    "* Linear models use a dense pipeline: median imputation (with missingness indicators), one-hot for categoricals, and standardization.\n",
    "\n",
    "* Tree/boosting models use a sparse pipeline (impute + OHE, no scaling).\n",
    "\n",
    "* CV uses GroupKFold grouped by STATE to avoid geographic leakage across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47f3a5b3-8cf1-4cfa-9f0c-c098f2ec0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward-compatible OHE kwargs (sklearn changed 'sparse' -> 'sparse_output')\n",
    "def ohe_kwargs(sparse=True):\n",
    "    try:\n",
    "        # sklearn >=1.2\n",
    "        return {\"handle_unknown\": \"ignore\", \"sparse_output\": sparse}\n",
    "    except TypeError:\n",
    "        # sklearn <1.2\n",
    "        return {\"handle_unknown\": \"ignore\", \"sparse\": sparse}\n",
    "\n",
    "# Linear: dense\n",
    "pre_linear = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\", add_indicator=True), numeric_base),\n",
    "    (\"cat\", OneHotEncoder(**ohe_kwargs(sparse=False)), categorical_base),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "scaler_dense = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "# Trees: sparse (keep memory/runtime low)\n",
    "pre_trees = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\", add_indicator=True), numeric_base),\n",
    "    (\"cat\", OneHotEncoder(**ohe_kwargs(sparse=True)), categorical_base),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# Grouped CV (by STATE)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "STATE_groups = train[\"STATE\"].values\n",
    "\n",
    "def cv_metrics(pipe, X, y, groups, n_splits=5):\n",
    "    splitter = GroupKFold(n_splits=n_splits)\n",
    "    r2  = cross_val_score(pipe, X, y,\n",
    "                          cv=splitter.split(X, y, groups=groups),\n",
    "                          scoring=\"r2\", n_jobs=1)\n",
    "    rm = -cross_val_score(pipe, X, y,\n",
    "                          cv=splitter.split(X, y, groups=groups),\n",
    "                          scoring=\"neg_root_mean_squared_error\", n_jobs=1)\n",
    "    return float(r2.mean()), float(r2.std()), float(rm.mean()), float(rm.std())\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    import numpy as np\n",
    "    return float(np.sqrt(((y_true - y_pred) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9caae2-aa3e-4775-99ea-e0c68aa40c03",
   "metadata": {},
   "source": [
    "### Baselines (Time-Aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e5808-6193-4144-bb3e-7035b267d714",
   "metadata": {},
   "source": [
    "We evaluate two simple, high-signal baselines for 2023:\n",
    "\n",
    "* Carry-forward (t−1): predict FI_prev when available; otherwise fallback to state mean.\n",
    "\n",
    "* State mean: average FI in 2010–2022 for the county’s state.\n",
    "\n",
    "These are strong yardsticks because FI is persistent year-to-year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1fc6a21-a904-4eb2-abfc-94fe73206dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseline_state': {'R2_Test': 0.42969340014717444, 'RMSE_Test': 0.029176112529187222}, 'baseline_carry': {'R2_Test': 0.9507582282520334, 'RMSE_Test': 0.01219221201734373}}\n"
     ]
    }
   ],
   "source": [
    "# State mean baseline\n",
    "state_mean = train.groupby(\"STATE\")[\"FI Rate\"].mean().to_dict()\n",
    "base_state = test[\"STATE\"].map(state_mean).fillna(train[\"FI Rate\"].mean()).values\n",
    "\n",
    "# Carry-forward baseline (use Year-1 per FIPS if available)\n",
    "prev = train[[\"FIPS\",\"Year\",\"FI Rate\"]].copy()\n",
    "prev[\"Year\"] = prev[\"Year\"] + 1\n",
    "carry = test[[\"FIPS\",\"Year\"]].merge(prev.rename(columns={\"FI Rate\":\"FI_prev\"}),\n",
    "                                    on=[\"FIPS\",\"Year\"], how=\"left\")\n",
    "base_carry = np.where(~np.isnan(carry[\"FI_prev\"].values),\n",
    "                      carry[\"FI_prev\"].values, base_state)\n",
    "\n",
    "print({\n",
    "    \"baseline_state\": {\"R2_Test\": float(np.corrcoef(y_test, base_state)[0,1]**2),\n",
    "                       \"RMSE_Test\": rmse(y_test, base_state)},\n",
    "    \"baseline_carry\": {\"R2_Test\": float(np.corrcoef(y_test, base_carry)[0,1]**2),\n",
    "                       \"RMSE_Test\": rmse(y_test, base_carry)}\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c238e29-0b12-4b15-8f21-83ff847de342",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb555147-3dc5-4b5c-8735-a93f90843ac1",
   "metadata": {},
   "source": [
    "We start with Ordinary Least Squares (OLS) under a standardized preprocessing pipeline (median imputation, one-hot encoding for categoricals, and scaling for dense linear models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f22b97-9a33-47cf-97e3-ee2531cc0257",
   "metadata": {},
   "source": [
    "### Model 1: All Features (OLS) (+FI_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149bd47-6dd1-42c9-a075-0250ab79ccdb",
   "metadata": {},
   "source": [
    "Design - Train a simple OLS model on all kept engineered features after train-window sparsity filtering. The goal is to set a linear baseline that uses broad cross-sectional signal without manual feature pruning.\n",
    "\n",
    "Evaluation - Metrics are reported for train (2010–2022), held-out test (2023), and grouped cross-validation by STATE (GroupKFold) to respect geographic dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737d03d3-bdb2-4e33-b616-bf752170b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'M1_OLS_AllFeatures(+FI_prev)', 'R2_Train': 0.9886119403754192, 'RMSE_Train': 0.0044216555242212715, 'R2_Test': 0.9949121193223383, 'RMSE_Test': 0.0025002975679884656, 'CV_R2_mean': 0.9735163297982499, 'CV_R2_std': 0.013733911482552555, 'CV_RMSE_mean': 0.00602389358164227, 'CV_RMSE_std': 0.0012609122655249713}\n"
     ]
    }
   ],
   "source": [
    "m1 = Pipeline([(\"pre\", Pipeline([(\"ct\", pre_linear), (\"sc\", StandardScaler())])),\n",
    "               (\"reg\", LinearRegression())])\n",
    "m1.fit(X_train_full, y_train)\n",
    "y_tr1 = m1.predict(X_train_full);\n",
    "y_te1 = m1.predict(X_test_full)\n",
    "\n",
    "M1 = {\"Model\":\"M1_OLS_AllFeatures(+FI_prev)\",\n",
    "      \"R2_Train\": r2_score(y_train,y_tr1), \n",
    "      \"RMSE_Train\": rmse(y_train,y_tr1),\n",
    "      \"R2_Test\":  r2_score(y_test, y_te1), \n",
    "      \"RMSE_Test\":  rmse(y_test, y_te1)}\n",
    "M1[\"CV_R2_mean\"],M1[\"CV_R2_std\"],M1[\"CV_RMSE_mean\"],M1[\"CV_RMSE_std\"] = cv_metrics(m1, X_train_full, y_train, STATE_groups)\n",
    "\n",
    "print(M1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f12062-5304-441e-96de-c75333e3f3b8",
   "metadata": {},
   "source": [
    "## Coefficient Peek — OLS (+FI_prev)\n",
    "\n",
    "We inspect the fitted OLS model’s coefficients to confirm that `FI_prev` is the dominant predictor and to see which covariates provide residual adjustments. We report both raw and absolute magnitudes and show the top drivers by |coef|.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74de8e07-de0b-4119-8170-e15c0f54c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_from_ct': 5547, 'n_from_pre': 5547, 'n_from_coef': 5547}\n",
      "Intercept (in standardized feature space): 0.141637\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num__prop_unsheltered</td>\n",
       "      <td>11.100401</td>\n",
       "      <td>11.100401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num__prop_sheltered</td>\n",
       "      <td>11.100375</td>\n",
       "      <td>11.100375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num__TOT_WHITE</td>\n",
       "      <td>-0.041660</td>\n",
       "      <td>0.041660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num__FI_prev</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.039750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num__TOT_MALE</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.025077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num__TOT_POP</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.019920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num__TOT_FEMALE</td>\n",
       "      <td>0.014944</td>\n",
       "      <td>0.014944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num__delta_FI_rate</td>\n",
       "      <td>0.012221</td>\n",
       "      <td>0.012221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num__TOT_BLACK</td>\n",
       "      <td>-0.009506</td>\n",
       "      <td>0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num__TOT_ASIAN</td>\n",
       "      <td>-0.008203</td>\n",
       "      <td>0.008203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num__missingindicator_delta_unemployment_rate</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>num__missingindicator_delta_FI_rate</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.004239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>num__TOT_LATINX</td>\n",
       "      <td>-0.002655</td>\n",
       "      <td>0.002655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num__percent_black</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>0.002150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num__percent_nonwhite</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>num__percent_white</td>\n",
       "      <td>-0.001799</td>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num__percent_latinx</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.001596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>num__missingindicator_restaurants_per_capita</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>num__missingindicator_grocery_per_capita</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>num__missingindicator_wholesale_per_capita</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>num__log_total_workforce</td>\n",
       "      <td>-0.001306</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>num__TOT_PACIFIC</td>\n",
       "      <td>-0.001284</td>\n",
       "      <td>0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>num__TOT_NATIVE</td>\n",
       "      <td>-0.000985</td>\n",
       "      <td>0.000985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>num__Employed</td>\n",
       "      <td>-0.000831</td>\n",
       "      <td>0.000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>num__log_cost_per_meal</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature       coef   abs_coef\n",
       "0                           num__prop_unsheltered  11.100401  11.100401\n",
       "1                             num__prop_sheltered  11.100375  11.100375\n",
       "2                                  num__TOT_WHITE  -0.041660   0.041660\n",
       "3                                    num__FI_prev   0.039750   0.039750\n",
       "4                                   num__TOT_MALE   0.025077   0.025077\n",
       "5                                    num__TOT_POP   0.019920   0.019920\n",
       "6                                 num__TOT_FEMALE   0.014944   0.014944\n",
       "7                              num__delta_FI_rate   0.012221   0.012221\n",
       "8                                  num__TOT_BLACK  -0.009506   0.009506\n",
       "9                                  num__TOT_ASIAN  -0.008203   0.008203\n",
       "10  num__missingindicator_delta_unemployment_rate   0.006971   0.006971\n",
       "11            num__missingindicator_delta_FI_rate  -0.004239   0.004239\n",
       "12                                num__TOT_LATINX  -0.002655   0.002655\n",
       "13                             num__percent_black  -0.002150   0.002150\n",
       "14                          num__percent_nonwhite   0.001799   0.001799\n",
       "15                             num__percent_white  -0.001799   0.001799\n",
       "16                            num__percent_latinx  -0.001596   0.001596\n",
       "17   num__missingindicator_restaurants_per_capita   0.001403   0.001403\n",
       "18       num__missingindicator_grocery_per_capita   0.001403   0.001403\n",
       "19     num__missingindicator_wholesale_per_capita   0.001403   0.001403\n",
       "20                       num__log_total_workforce  -0.001306   0.001306\n",
       "21                               num__TOT_PACIFIC  -0.001284   0.001284\n",
       "22                                num__TOT_NATIVE  -0.000985   0.000985\n",
       "23                                  num__Employed  -0.000831   0.000831\n",
       "24                         num__log_cost_per_meal   0.000823   0.000823"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Rows containing 'FI_prev' --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num__FI_prev</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.039750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>num__missingindicator_FI_prev</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature      coef  abs_coef\n",
       "3                    num__FI_prev  0.039750  0.039750\n",
       "39  num__missingindicator_FI_prev  0.000467  0.000467"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === OLS coefficients from a scikit-learn Pipeline (standardized) ==================\n",
    "# Assumes you already have a fitted pipeline named `m1`\n",
    "\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Pull fitted steps\n",
    "pre = m1.named_steps[\"pre\"]                 # the inner Pipeline with (\"ct\", ...), (\"sc\", ...)\n",
    "ct  = pre.named_steps[\"ct\"]                 # ColumnTransformer\n",
    "reg = m1.named_steps[\"reg\"]                 # LinearRegression\n",
    "\n",
    "# Coefficients (standardized-space, because we used StandardScaler)\n",
    "coefs = np.asarray(reg.coef_).ravel()\n",
    "intercept = float(reg.intercept_)\n",
    "\n",
    "# Try to get feature names from the fitted ColumnTransformer (handles OHE + indicators)\n",
    "try:\n",
    "    feat_names = ct.get_feature_names_out()\n",
    "except Exception:\n",
    "    # Fallback: generate generic names that exactly match transformed column count\n",
    "    # (pre.transform builds the actual design matrix that fed the regressor)\n",
    "    n_out = pre.transform(X_train_full.iloc[:5]).shape[1]\n",
    "    feat_names = np.array([f\"f_{i}\" for i in range(n_out)])\n",
    "\n",
    "# Sanity checks (these should all be equal)\n",
    "n_from_ct   = len(feat_names)\n",
    "n_from_pre  = pre.transform(X_train_full.iloc[:5]).shape[1]\n",
    "n_from_coef = coefs.shape[0]\n",
    "print({\"n_from_ct\": n_from_ct, \"n_from_pre\": n_from_pre, \"n_from_coef\": n_from_coef})\n",
    "\n",
    "# If any mismatch sneaks in, hard-fix names to avoid length errors\n",
    "if not (n_from_ct == n_from_pre == n_from_coef):\n",
    "    feat_names = np.array([f\"f_{i}\" for i in range(n_from_coef)])\n",
    "\n",
    "# Build tidy coefficient table\n",
    "coef_df = (\n",
    "    pd.DataFrame({\"feature\": feat_names, \"coef\": coefs})\n",
    "      .assign(abs_coef=lambda d: d[\"coef\"].abs())\n",
    "      .sort_values(\"abs_coef\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Intercept (in standardized feature space): {intercept:,.6f}\")\n",
    "display(coef_df.head(25))  # Top 25 by |coef|\n",
    "\n",
    "# Quick lookups for specific features of interest (e.g., FI_prev)\n",
    "mask_fiprev = coef_df[\"feature\"].str.contains(\"FI_prev\", regex=False)\n",
    "print(\"\\n-- Rows containing 'FI_prev' --\")\n",
    "display(coef_df[mask_fiprev])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4094eb2-5c70-42bb-940f-791621f029fb",
   "metadata": {},
   "source": [
    "### Model 2 — OLS with VIF≤10 (+FI_prev)\n",
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1bb17-5c9e-4118-b994-a612e63b484f",
   "metadata": {},
   "source": [
    "We compute VIF on train numerics (after imputation) and iteratively drop the worst offender until all retained numerics have VIF ≤ 10. Categoricals are kept. This improves linear stability/interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d812cf88-0d87-4200-9b71-2a132cd8e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'M2_OLS_VIF≤10(+FI_prev,n_num=32)', 'R2_Train': 0.988603045589448, 'RMSE_Train': 0.004423381981760407, 'R2_Test': 0.9946317156246003, 'RMSE_Test': 0.0025682718790255195, 'CV_R2_mean': 0.975714286099141, 'CV_R2_std': 0.012120081656483605, 'CV_RMSE_mean': 0.005808382231404444, 'CV_RMSE_std': 0.001166762562027063}\n"
     ]
    }
   ],
   "source": [
    "# M2: OLS with VIF-filtered numerics (recompute VIF on numeric_base that includes FI_prev)\n",
    "\n",
    "num_imp = SimpleImputer(strategy=\"median\").fit_transform(X_train_full[numeric_base])\n",
    "Xv = pd.DataFrame(num_imp, columns=numeric_base)\n",
    "keep = numeric_base.copy()\n",
    "changed = True\n",
    "while changed and len(keep) > 2:\n",
    "    X_vif = sm.add_constant(Xv[keep])\n",
    "    vifs = pd.Series([variance_inflation_factor(X_vif.values, i+1) for i in range(len(keep))], index=keep)\n",
    "    worst = vifs.sort_values(ascending=False)\n",
    "    if worst.iloc[0] > 10:\n",
    "        keep.remove(worst.index[0])\n",
    "    else:\n",
    "        changed = False\n",
    "\n",
    "numeric_vif = keep\n",
    "pre_vif = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\", add_indicator=True), numeric_vif),\n",
    "    (\"cat\", OneHotEncoder(**ohe_kwargs(sparse=False)), categorical_base),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "m2 = Pipeline([(\"pre\", Pipeline([(\"ct\", pre_vif), \n",
    "                (\"sc\", StandardScaler())])),\n",
    "               (\"reg\", LinearRegression())])\n",
    "m2.fit(train[numeric_vif+categorical_base], y_train)\n",
    "\n",
    "y_tr2 = m2.predict(train[numeric_vif+categorical_base]); \n",
    "y_te2 = m2.predict(test[numeric_vif+categorical_base])\n",
    "\n",
    "M2 = {\"Model\":f\"M2_OLS_VIF≤10(+FI_prev,n_num={len(numeric_vif)})\",\n",
    "      \"R2_Train\": r2_score(y_train,y_tr2),\n",
    "      \"RMSE_Train\": rmse(y_train,y_tr2),\n",
    "      \"R2_Test\":  r2_score(y_test, y_te2), \n",
    "      \"RMSE_Test\":  rmse(y_test, y_te2)}\n",
    "M2[\"CV_R2_mean\"],M2[\"CV_R2_std\"],M2[\"CV_RMSE_mean\"],M2[\"CV_RMSE_std\"] = cv_metrics(m2, train[numeric_vif+categorical_base], y_train, STATE_groups)\n",
    "\n",
    "print(M2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237343c-772e-4ee6-89a5-bb17a40d8920",
   "metadata": {},
   "source": [
    "### Tree / Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db2387-2c6a-402a-9729-b2772ea0f461",
   "metadata": {},
   "source": [
    "Nonlinear methods can capture interaction and curvature absent from OLS. We fit:\n",
    "\n",
    "* Random Forest Regressor (bagged trees, robust to monotone transforms and interactions),\n",
    "\n",
    "* XGBoost Regressor (gradient boosting with regularization),\n",
    "\n",
    "* CatBoost Regressor (ordered boosting; handles categorical structure well).\n",
    "\n",
    "All use the sparse preprocessor (impute + OHE). Metrics are reported on the 2023 hold-out and via grouped CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a1eaf-bd56-49dd-8061-9b2e7053c629",
   "metadata": {},
   "source": [
    "#### Model 3 — XGBoost (+FI_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac34a45a-fef2-4d32-9a54-a36d03b2c920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'M3_XGBoost(+FI_prev)', 'R2_Train': 0.9929356926886994, 'RMSE_Train': 0.003482529715678512, 'R2_Test': 0.9932093383856818, 'RMSE_Test': 0.0028885459763161223, 'CV_R2_mean': 0.9815750833340721, 'CV_R2_std': 0.004140269530218133, 'CV_RMSE_mean': 0.005357589545220867, 'CV_RMSE_std': 0.0011907174977523211}\n"
     ]
    }
   ],
   "source": [
    "M3 = None\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    m3 = Pipeline([(\"pre\", pre_trees),\n",
    "                   (\"reg\", xgb.XGBRegressor(\n",
    "                       objective=\"reg:squarederror\",\n",
    "                       n_estimators=600, learning_rate=0.06,\n",
    "                       max_depth=6, subsample=0.9, colsample_bytree=0.9,\n",
    "                       min_child_weight=3, tree_method=\"hist\",\n",
    "                       n_jobs=-1, random_state=42))])\n",
    "    m3.fit(X_train_full, y_train)\n",
    "    y_tr3 = np.clip(m3.predict(X_train_full),0,1); \n",
    "    y_te3 = np.clip(m3.predict(X_test_full),0,1)\n",
    "    M3 = {\"Model\":\"M3_XGBoost(+FI_prev)\",\n",
    "          \"R2_Train\": r2_score(y_train,y_tr3),\n",
    "          \"RMSE_Train\": rmse(y_train,y_tr3),\n",
    "          \"R2_Test\":  r2_score(y_test, y_te3),\n",
    "          \"RMSE_Test\":  rmse(y_test, y_te3)}\n",
    "    M3[\"CV_R2_mean\"],M3[\"CV_R2_std\"],M3[\"CV_RMSE_mean\"],M3[\"CV_RMSE_std\"] = cv_metrics(m3, X_train_full, y_train, STATE_groups)\n",
    "except Exception as e:\n",
    "    print(\"XGB skipped:\", e)\n",
    "\n",
    "\n",
    "print(M3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00baf3-fcc0-4377-aa17-c607d67cca21",
   "metadata": {},
   "source": [
    "#### Model 4 — CatBoost (+FI_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa56fe74-1f75-4d8f-a723-cc4cea383d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'M4_CatBoost(+FI_prev)', 'R2_Train': 0.9909000541857029, 'RMSE_Train': 0.003952568836541482, 'R2_Test': 0.9849605784665043, 'RMSE_Test': 0.004298716513538617, 'CV_R2_mean': 0.984097819396313, 'CV_R2_std': 0.0037909584690865306, 'CV_RMSE_mean': 0.00493392901834733, 'CV_RMSE_std': 0.0008607139448113483}\n"
     ]
    }
   ],
   "source": [
    "M4 = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    m4 = Pipeline([(\"pre\", pre_trees),\n",
    "                   (\"reg\", CatBoostRegressor(loss_function=\"RMSE\",\n",
    "                                             iterations=800, learning_rate=0.05, depth=8,\n",
    "                                             l2_leaf_reg=6, subsample=0.9,\n",
    "                                             od_type=\"Iter\", od_wait=50,\n",
    "                                             random_seed=42, verbose=False))])\n",
    "    m4.fit(X_train_full, y_train)\n",
    "    y_tr4 = np.clip(m4.predict(X_train_full),0,1);\n",
    "    y_te4 = np.clip(m4.predict(X_test_full),0,1)\n",
    "    M4 = {\"Model\":\"M4_CatBoost(+FI_prev)\",\n",
    "          \"R2_Train\": r2_score(y_train,y_tr4),\n",
    "          \"RMSE_Train\": rmse(y_train,y_tr4),\n",
    "          \"R2_Test\":  r2_score(y_test, y_te4),\n",
    "          \"RMSE_Test\":  rmse(y_test, y_te4)}\n",
    "    M4[\"CV_R2_mean\"],M4[\"CV_R2_std\"],M4[\"CV_RMSE_mean\"],M4[\"CV_RMSE_std\"] = cv_metrics(m4, X_train_full, y_train, STATE_groups)\n",
    "except Exception as e:\n",
    "    print(\"CatBoost skipped:\", e)\n",
    "\n",
    "print(M4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fa487-0418-4890-90cc-75f34e9450c8",
   "metadata": {},
   "source": [
    "#### Model 5 — Random Forest (+FI_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c8c923-a1b0-47a8-97a9-5bc04bc7953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- M5: Random Forest (fast, solid baseline) ---\n",
    "\n",
    "m5 = Pipeline([(\"pre\", pre_trees),\n",
    "               (\"reg\", RandomForestRegressor(\n",
    "                   n_estimators=700, min_samples_leaf=2,\n",
    "                   n_jobs=-1, random_state=42))])\n",
    "m5.fit(X_train_full, y_train)\n",
    "y_tr5 = np.clip(m5.predict(X_train_full),0,1);\n",
    "y_te5 = np.clip(m5.predict(X_test_full),0,1)\n",
    "\n",
    "M5 = {\"Model\":\"M5_RandomForest(+FI_prev)\",\n",
    "      \"R2_Train\": r2_score(y_train,y_tr5),\n",
    "      \"RMSE_Train\": rmse(y_train,y_tr5),\n",
    "      \"R2_Test\":  r2_score(y_test, y_te5),\n",
    "      \"RMSE_Test\":  rmse(y_test, y_te5)}\n",
    "M5[\"CV_R2_mean\"],M5[\"CV_R2_std\"],M5[\"CV_RMSE_mean\"],M5[\"CV_RMSE_std\"] = cv_metrics(m5, X_train_full, y_train, STATE_groups)\n",
    "\n",
    "print(M5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b921ee2-7e4e-462b-91a6-a027569f9727",
   "metadata": {},
   "source": [
    "### Hybrid Stack (+ State Residual Correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ced093-f0ae-42cb-90d4-37c85e2f634e",
   "metadata": {},
   "source": [
    "We stack complementary models (OLS + boosting/trees) by training a Ridge meta-model on out-of-fold predictions from the train window.\n",
    "\n",
    "We also add a simple state residual correction learned on train OOF residuals and applied to 2023 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae1fe4-00af-4da9-ab20-d9e7454fece6",
   "metadata": {},
   "source": [
    "#### Model 6 — Hybrid Stack (+FI_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "010a479a-96ac-4b1c-8f49-2c49a8965923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'M6_Hybrid(M1_OLS+M3_XGB+M4_CatB)+StateCorr(+FI_prev)', 'R2_Train': nan, 'RMSE_Train': nan, 'R2_Test': 0.993813919315895, 'RMSE_Test': 0.00275696358445994, 'CV_R2_mean': nan, 'CV_R2_std': nan, 'CV_RMSE_mean': nan, 'CV_RMSE_std': nan}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Gather available base models & their preds\n",
    "base_models = []\n",
    "base_names  = []\n",
    "# Always include M1 (OLS)\n",
    "base_models.append(m1); base_names.append(\"M1_OLS\")\n",
    "# Include XGB/CatBoost if they exist\n",
    "if M3 is not None: base_models.append(m3); base_names.append(\"M3_XGB\")\n",
    "if M4 is not None: base_models.append(m4); base_names.append(\"M4_CatB\")\n",
    "\n",
    "M6 = None\n",
    "if base_models:\n",
    "    oof = pd.DataFrame(index=train.index, columns=base_names, dtype=float)\n",
    "    splitter = GroupKFold(n_splits=5)\n",
    "    for tr_idx, va_idx in splitter.split(X_train_full, y_train, groups=STATE_groups):\n",
    "        Xtr_f, ytr_f = X_train_full.iloc[tr_idx], y_train[tr_idx]\n",
    "        Xva_f = X_train_full.iloc[va_idx]\n",
    "        for name, mdl in zip(base_names, base_models):\n",
    "            mdl.fit(Xtr_f, ytr_f)\n",
    "            oof.iloc[va_idx, oof.columns.get_loc(name)] = np.clip(mdl.predict(Xva_f),0,1)\n",
    "    test_stack = pd.DataFrame(index=test.index, columns=base_names, dtype=float)\n",
    "    for name, mdl in zip(base_names, base_models):\n",
    "        mdl.fit(X_train_full, y_train)\n",
    "        test_stack[name] = np.clip(mdl.predict(X_test_full),0,1)\n",
    "\n",
    "    meta = Ridge(alpha=1.0, random_state=42)\n",
    "    meta.fit(oof.values, y_train)\n",
    "    pred_stack = np.clip(meta.predict(test_stack.values), 0, 1)\n",
    "\n",
    "    # simple state residual correction\n",
    "    train_oof = train.copy()\n",
    "    train_oof[\"oof_pred\"] = np.clip(meta.predict(oof.values), 0, 1)\n",
    "    train_oof[\"residual\"] = train_oof[\"FI Rate\"] - train_oof[\"oof_pred\"]\n",
    "    state_corr = train_oof.groupby(\"STATE\")[\"residual\"].mean()\n",
    "\n",
    "    hyb = np.clip(pred_stack + test[\"STATE\"].map(state_corr).fillna(0.0).values, 0, 1)\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    M6 = {\"Model\": f\"M6_Hybrid({'+'.join(base_names)})+StateCorr(+FI_prev)\",\n",
    "          \"R2_Train\": np.nan, \"RMSE_Train\": np.nan,\n",
    "          \"R2_Test\": r2_score(y_test, hyb), \"RMSE_Test\": rmse(y_test, hyb),\n",
    "          \"CV_R2_mean\": np.nan, \"CV_R2_std\": np.nan,\n",
    "          \"CV_RMSE_mean\": np.nan, \"CV_RMSE_std\": np.nan}\n",
    "\n",
    "print(M6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86809caa-ae13-4f11-89c7-9e4c625b4dbe",
   "metadata": {},
   "source": [
    "#### Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debbc7d-41c9-469c-ba29-dc9b078fda2f",
   "metadata": {},
   "source": [
    "We consolidate all models into a single table with:\n",
    "Model, R2_Train, RMSE_Train, R2_Test, RMSE_Test, CV_R2_mean, CV_R2_std, CV_RMSE_mean, CV_RMSE_std\n",
    "\n",
    "Sorted by RMSE_Test (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b4a0d2-76af-4e5c-be74-49d86b45c85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2_Train</th>\n",
       "      <th>RMSE_Train</th>\n",
       "      <th>R2_Test</th>\n",
       "      <th>RMSE_Test</th>\n",
       "      <th>CV_R2_mean</th>\n",
       "      <th>CV_R2_std</th>\n",
       "      <th>CV_RMSE_mean</th>\n",
       "      <th>CV_RMSE_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1_OLS_AllFeatures(+FI_prev)</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2_OLS_VIF≤10(+FI_prev,n_num=32)</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M6_Hybrid(M1_OLS+M3_XGB+M4_CatB)+StateCorr(+FI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M3_XGBoost(+FI_prev)</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M5_RandomForest(+FI_prev)</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M4_CatBoost(+FI_prev)</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  R2_Train  RMSE_Train  \\\n",
       "0                       M1_OLS_AllFeatures(+FI_prev)     0.989      0.0044   \n",
       "1                   M2_OLS_VIF≤10(+FI_prev,n_num=32)     0.989      0.0044   \n",
       "2  M6_Hybrid(M1_OLS+M3_XGB+M4_CatB)+StateCorr(+FI...       NaN         NaN   \n",
       "3                               M3_XGBoost(+FI_prev)     0.993      0.0035   \n",
       "4                          M5_RandomForest(+FI_prev)     0.995      0.0029   \n",
       "5                              M4_CatBoost(+FI_prev)     0.991      0.0040   \n",
       "\n",
       "   R2_Test  RMSE_Test  CV_R2_mean  CV_R2_std  CV_RMSE_mean  CV_RMSE_std  \n",
       "0    0.995     0.0025       0.974      0.014        0.0060       0.0013  \n",
       "1    0.995     0.0026       0.976      0.012        0.0058       0.0012  \n",
       "2    0.994     0.0028         NaN        NaN           NaN          NaN  \n",
       "3    0.993     0.0029       0.982      0.004        0.0054       0.0012  \n",
       "4    0.993     0.0029         NaN        NaN           NaN          NaN  \n",
       "5    0.985     0.0043       0.984      0.004        0.0049       0.0009  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [M1, M2, M5]\n",
    "if M3 is not None: rows.append(M3)\n",
    "if M4 is not None: rows.append(M4)\n",
    "if M6 is not None: rows.append(M6)\n",
    "\n",
    "results = (pd.DataFrame(rows)\n",
    "           .loc[:, [\"Model\",\"R2_Train\",\"RMSE_Train\",\"R2_Test\",\"RMSE_Test\",\"CV_R2_mean\",\"CV_R2_std\",\"CV_RMSE_mean\",\"CV_RMSE_std\"]]\n",
    "           .sort_values(\"RMSE_Test\")\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "# Pretty rounding for the report\n",
    "results_rounded = results.copy()\n",
    "for col in [\"R2_Train\",\"R2_Test\",\"CV_R2_mean\",\"CV_R2_std\"]:\n",
    "    results_rounded[col] = results_rounded[col].astype(float).round(3)\n",
    "for col in [\"RMSE_Train\",\"RMSE_Test\",\"CV_RMSE_mean\",\"CV_RMSE_std\"]:\n",
    "    results_rounded[col] = results_rounded[col].astype(float).round(4)\n",
    "\n",
    "results_rounded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13ed51-efd4-4760-8cec-f869156508e7",
   "metadata": {},
   "source": [
    "### Modeling Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc2354-8776-4ac6-a9ff-934541c671f1",
   "metadata": {},
   "source": [
    "Our time-aware modeling shows that county FI rates are highly persistent year-to-year. Incorporating a simple lag feature (FI_prev) plus a few macro covariates yields excellent accuracy:\n",
    "\n",
    "    * Best model (OLS, all features + FI_prev) achieved R² ≈ 0.995 and RMSE ≈ 0.0025 on the 2023 hold-out, with 5-fold state-grouped CV R² ≈ 0.974 (CV RMSE ≈ 0.0060).\n",
    "    \n",
    "    * Tree/boosting baselines with the same inputs also performed strongly (e.g., XGBoost R² ≈ 0.993, Random Forest R² ≈ 0.993), while CatBoost trailed slightly (R² ≈ 0.985).\n",
    "\n",
    "Interpretability:-\n",
    "\n",
    "  Coefficient inspection confirms that last year’s FI (FI_prev) is the dominant predictor, with meaningful—but smaller—contributions from unemployment and cost per meal. This aligns with domain expectations: FI evolves gradually and tracks labor market and affordability conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce27a55-1735-4bb4-aa9a-e1ce4f6ab9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
